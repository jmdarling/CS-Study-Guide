#Big O Notation
"The language we use for articulating how long an algorithm takes to run." [1]

##Introduction
Big O Notation is used to "express the runtime [of an algorithm] in terms of **how quickly the runtime grows relative to the input as the input gets arbitrarily large**. [1]"


This is all a fancy way to say that we are looking at abstract measures here, not concrete numbers. There are too many varying factors in computing to use concrete numbers. Not all processors run at the same speed, not all computers have the same amount of available memory, etc. There are infinite possible scenarios in which an algorithm could be running on any given computer. We can't use a concrete number to describe the performance of an algorithm because that number would only be applicable if the algorithm were to be run under the exact same circumstances every time.

What it all boils down to is that we use Big O to measure the relative complexity of an algorithm.

##Complexities
There are six "classes" of complexity that can be represented by Big O Notation:


##Examples
The easiest way to understand Big O Notation is simply to look at examples, so that's exactly what we are going to do.

##References
[1] https://www.interviewcake.com/big-o-notation-time-and-space-complexity

[2] http://www.corejavainterviewquestions.com/idiots-guide-big-o/
